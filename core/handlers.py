import tempfile

from aiogram import Router, types, F
from aiogram.filters import CommandStart

from core.bot_core import logger, bot, BOT_USERNAME, ADMIN_ID, dp
from utils import llm_utils, stt_utils
from utils.logging_utils import log_message

start_router = Router()
SUPPORTED_TYPES = ["text", "voice"]
LLM_MODELS = {
    "llama-3.1-8b-instant": {
        "name": "Llama 3.1 8B",
    },
    "llama-3.3-70b-versatile": {
        "name": "Llama 3.3 70B",
    },
    "openai/gpt-oss-120b": {
        "name": "GPT OSS 120B",
    },
    "openai/gpt-oss-20b": {
        "name": "GPT OSS 20B",
    },
    "meta-llama/llama-4-maverick-17b-128e-instruct": {
        "name": "Llama 4 Maverick 17B 128E",
    },
    "meta-llama/llama-4-scout-17b-16e-instruct": {
        "name": "Llama 4 Scout 17B 16E",
    },
    "qwen/qwen3-32b": {
        "name": "Qwen3 32B",
    },
    "moonshotai/kimi-k2-instruct-0905": {
        "name": "Kimi K2 0905",
    },
}


@start_router.message(CommandStart())
async def cmd_start(message: types.Message):
    user = message.from_user
    username = f"@{user.username}" if user.username else f"{user.first_name or user.id}"
    uid = message.from_user.id

    welcome_text = (
        f"ü§ì *Nerdinzzz* ‚Äì –≤–∞—à —É–º–Ω—ã–π —á–∞—Ç-–±–æ—Ç –Ω–∞ –±–∞–∑–µ `{LLM_MODELS[dp['llm']]["name"]}`!\n\n"
        "–û–Ω –º–≥–Ω–æ–≤–µ–Ω–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —É–º–µ–µ—Ç –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—å –≥–æ–ª–æ—Å–æ–≤—ã–µ –≤ —Ç–µ–∫—Å—Ç.\n\n"
        "–ü—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—à–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≥–æ–ª–æ—Å–æ–≤–æ–µ, –∏ –±–æ—Ç —Å—Ä–∞–∑—É –¥–∞—Å—Ç –æ—Ç–≤–µ—Ç!\n\n"
        "üîó –î–æ–±–∞–≤—å—Ç–µ –±–æ—Ç–∞, —Ä–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –∫ —Å–æ–æ–±—â–µ–Ω–∏—è–º, –∏ –æ–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:\n"
        "   ‚Ä¢ –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≥–æ–ª–æ—Å–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç\n"
        f"   ‚Ä¢ –û—Ç–≤–µ—Ç–∏—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –µ—Å–ª–∏ —É–ø–æ–º—è–Ω—É—Ç—å `@{BOT_USERNAME}`"
    )

    await message.answer(welcome_text, parse_mode="Markdown")
    logger.info(f"User {username} ({uid}) started the bot")


@start_router.message(F.content_type == "voice")
async def voice_handler(message: types.Message):
    voice = message.voice
    file = await bot.get_file(voice.file_id)

    with tempfile.NamedTemporaryFile(suffix=".ogg") as tmp:
        await bot.download_file(
            file.file_path,
            destination=tmp.name
        )
        stt_response = await stt_utils.stt(tmp.name)

        log_message(message=message, stt_response=stt_response)
        await message.reply(stt_response)


@start_router.message(F.content_type == "video_note")
async def video_note_handler(message: types.Message):
    video_note = message.video_note
    file = await bot.get_file(video_note.file_id)

    with tempfile.NamedTemporaryFile(suffix=".mp4") as tmp:
        await bot.download_file(
            file.file_path,
            destination=tmp.name)
        stt_response = await stt_utils.stt_from_video(tmp.name)

        log_message(message=message, stt_response=stt_response)
        await message.reply(stt_response)


# text + group + mention
@start_router.message(F.content_type == "text",
                      lambda m: m.chat.type in ("group", "supergroup"),  # –≥—Ä—É–ø–ø—ã
                      lambda m: m.text and m.text.startswith(f"@{BOT_USERNAME} "))
async def text_group_handler(message: types.Message):
    text = message.text.replace(f'@{BOT_USERNAME} ', '').strip()
    if not text: return
    llm_response = await llm_utils.get_llm_response(text)

    log_message(message=message, llm_response=llm_response)
    await message.reply(llm_response, parse_mode=None)


@start_router.message(F.content_type == "text", F.chat.type == "private")
async def text_private_handler(message: types.Message):
    text = message.text
    if message.from_user.id == ADMIN_ID and text.startswith(f"/set_llm "):
        log_message(message=message)
        model_code = text.replace("/set_llm ", '').strip()
        if model_code not in LLM_MODELS:
            await message.answer(f'–¢–∞–∫–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ—Ç –≤ –Ω–∞–±–æ—Ä–µ‚ùå')
            return
        dp['llm'] = model_code
        await message.answer(f'–ú–æ–¥–µ–ª—å  `{LLM_MODELS[model_code]["name"]}`  —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞‚úÖ', parse_mode="Markdown")
        logger.warning(f"LLM changed to {model_code}")
        return

    llm_response = await llm_utils.get_llm_response(text)
    log_message(message=message, llm_response=llm_response)
    await message.answer(llm_response, parse_mode=None)


@start_router.message(~F.content_type.in_(SUPPORTED_TYPES), F.chat.type == "private")
async def unsupported_handler(message: types.Message):
    log_message(message=message)
    await message.answer("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Å–æ–æ–±—â–µ–Ω–∏—è")
