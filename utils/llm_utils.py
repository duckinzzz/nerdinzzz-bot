from groq import AsyncGroq

from core.bot_core import LLM_TOKEN

client = AsyncGroq(api_key=LLM_TOKEN)
llm_model_name = "OpenAI/GPT-OSS-120b"

system_prompt = (
    f"Ð¢Ñ‹ â€” Nerdinzzz ðŸ¤“, LLM Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚ Ð½Ð° Ð±Ð°Ð·Ðµ {llm_model_name}. "
    "Ð¢Ð²Ð¾Ð¹ ÑÐ¾Ð·Ð´Ð°Ñ‚ÐµÐ»ÑŒ - @duckinzzz. "
    "Ð¢Ñ‹ ÑƒÐ¼ÐµÐµÑˆÑŒ Ð±Ñ‹ÑÑ‚Ñ€Ð¾ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¸ Ð¿Ñ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ñ‹Ð²Ð°Ñ‚ÑŒ Ð³Ð¾Ð»Ð¾ÑÐ¾Ð²Ñ‹Ðµ Ð² Ñ‚ÐµÐºÑÑ‚. "
    "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ ÐºÑ€Ð°Ñ‚ÐºÐ¾, ÑÑÐ½Ð¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ, Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 3-4 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ. "
    "Ð•ÑÐ»Ð¸ Ð½ÑƒÐ¶ÐµÐ½ ÑÐ¿Ð¸ÑÐ¾Ðº - Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿ÑƒÐ½ÐºÑ‚Ñ‹. "
    "ÐÐµ Ð·Ð°Ð´Ð°Ð²Ð°Ð¹ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð½Ñ‹Ñ… Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð², Ð½Ðµ Ð¾Ð±ÑŠÑÑÐ½ÑÐ¹ ÑÐ²Ð¾Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ."
)


async def get_llm_response(user_prompt: str) -> str:
    messages = [
        {
            "role": "system",
            "content": system_prompt.lower()
        },
        {"role": "user", "content": user_prompt}
    ]

    completion = await client.chat.completions.create(model=llm_model_name, messages=messages, temperature=1,
                                                      max_completion_tokens=8192, top_p=1, reasoning_effort="medium",
                                                      stream=False, stop=None)

    return completion.choices[0].message.content
