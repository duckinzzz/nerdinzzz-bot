from groq import AsyncGroq

from core.bot_core import LLM_TOKEN

client = AsyncGroq(api_key=LLM_TOKEN)
system_prompt = (
    "Ð¢Ñ‹ â€” Nerdinzzz ðŸ¤“, LLM Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚ Ð½Ð° Ð±Ð°Ð·Ðµ openai/gpt-oss-120b. "
    "Ð¢Ñ‹ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑˆÑŒ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾, ÑÑÐ½Ð¾ Ð¸ Ð¿Ð¾ Ð´ÐµÐ»Ñƒ. "
    "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð½Ðµ Ð±Ð¾Ð»ÑŒÑˆÐµ 2â€“3 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹, Ð¸Ð·Ð±ÐµÐ³Ð°Ð¹ Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹ Ð¸ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð¾Ð², ÐµÑÐ»Ð¸ Ð¸Ñ… Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚. "
    "Ð•ÑÐ»Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿ÑƒÐ½ÐºÑ‚Ñ‹. "
    "ÐÐµ Ð·Ð°Ð´Ð°Ð²Ð°Ð¹ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð½Ñ‹Ñ… Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ¾Ð², Ð½Ðµ Ð¾Ð±ÑŠÑÑÐ½ÑÐ¹ ÑÐ²Ð¾Ð¸ Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ."
)

llm_model_name = "openai/gpt-oss-120b"


async def get_llm_response(user_prompt: str) -> str:
    messages = [
        {
            "role": "system",
            "content": system_prompt
        },
        {"role": "user", "content": user_prompt}
    ]

    completion = await client.chat.completions.create(
        model=llm_model_name,
        messages=messages,
        temperature=1,
        max_completion_tokens=8192,
        top_p=1,
        reasoning_effort="medium",
        stream=False,
        stop=None
    )

    return completion.choices[0].message.content
